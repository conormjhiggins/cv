This is where you enter all of your CV entries. Each entry should have a section identifier, whether to include in resume, a title, location, institution, start/end dates, and up to 3 description bullets.
section,in_resume,title,loc,institution,start,end,description_1,description_2,description_3
education,TRUE,MSc Statistics,Lancaster,Lancaster University,2018,2019,"Merit awarded. [Dissertation: 'Dynamic Bayesian models for predicting Premier League Football outcomes'](https://github.com/conormjhiggins/masters-dissertation/blob/main/masters_dissertation.pdf), applying advanced Bayesian statistical modelling to sports performance data.",,
education,TRUE,BSc Mathematics with Statistics,Lancaster,Lancaster University,2015,2018,"Strong quantitative foundation in statistics, probability and mathematical modelling; minor in Chemistry.",,
industry_positions,TRUE,Senior Analytics Engineer,Remote,Deliveroo,2025,Current,"Built a Snowflake and Python platform that ingests ML model outputs to profile every restaurant on Deliveroo's global marketplace, forecast their monthly revenue, and surface the highest-value targets for the commercial sales team.","Cut pipeline runtime by 50% (from over 4 hours to under 2) by systematically refactoring hundreds of models and rethinking compute allocation, bringing substantial Snowflake cost savings along the way.","Got the Analytics Engineering function properly set up with AI tooling, writing practical guides and running workshops on Claude and Cursor for analysts and stakeholders; also acting as the on-call escalation engineer for major data incidents and an interviewer for potential candidates."
industry_positions,TRUE,Analytics Engineer,London,Deliveroo,2022,2025,"Lead AE for Deliveroo's restaurant domain, owning the full data stack from ELT pipelines through to dimensional models and the reporting layer that commercial teams relied on daily across all markets.","Worked closely with Sales, Science and Product to keep the data behind Deliveroo's most critical Looker dashboards accurate and reliable for senior stakeholders.","Wrote and maintained production ELT pipelines using a custom in-house orchestration framework, and built Python tooling for pipeline automation, data quality testing and output validation."
industry_positions,TRUE,Analytics Engineer,London,DAZN,2020,2022,"Built DAZN's first payments data warehouse from scratch on Snowflake and Airflow, creating the company's only source of truth for invoices, transactions and involuntary churn reporting across a fast-growing global streaming platform.","Wrote and maintained ELT pipelines using dbt, Airflow and Python; worked with PMs and stakeholders to translate data into product and commercial decisions.", ""
